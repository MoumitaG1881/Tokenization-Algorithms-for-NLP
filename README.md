# Tokenization-Algorithms-for-NLP
Creating various tokenization algorithms

Description: Write a Python function to implement a basic tokenization algorithm for a given language.
Guidelines: You can choose any language as you like. Note: GUI is not required.

Brief Description: Tokenization is a fundamental and crucial step in text processing and natural language processing (NLP), transforming raw text into compliant units for analysis.

Note: In this task, I have considered English language as I have the choice to select any language for making a basic tokenization algorithm (as per the task's instruction).

I have done the task by using four methods:

1. By using NLTK’s word_tokenize() method
2. By using str.split() method
3. By using Regex with re.findall() method
4. By using Gensim’s tokenize() method

Tokenization algorithms are created by using Python language in Jupyter Notebook.

**Note**
Please follow the .ipynb fille (attached under 'Tokenization Algorithms for NLP' repository). A detailed documentation is provided there for seamless understanding of each tokenization algorithm and their use cases.
