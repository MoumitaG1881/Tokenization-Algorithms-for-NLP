{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dca3bf-d460-40d2-9f60-a331d3b7b138",
   "metadata": {},
   "source": [
    "## Creating basic tokenizer\n",
    "\n",
    "Description: Write a Python function to implement a basic tokenization algorithm for a given language.\\\n",
    "Guidelines: You can choose any language as you like. \n",
    "Note: GUI is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c63b0-7544-4fa3-a3f1-1e583617b350",
   "metadata": {},
   "source": [
    "Brief Description: Tokenization is a fundamental and crucial step in text processing and natural language processing (NLP), transforming raw text into compliant units for analysis.\n",
    "\n",
    "Note: In this task, I have considered English language as I have the choice to select any language for making a basic tokenization algorithm (as per the task's instruction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f74f1d-6c06-4b19-8914-85dc9d5131d0",
   "metadata": {},
   "source": [
    "I have done the task by using four methods:\n",
    "\n",
    "1. By using NLTK’s word_tokenize() method\n",
    "2. By using str.split() method\n",
    "3. By using Regex with re.findall() method\n",
    "4. By using Gensim’s tokenize() method\n",
    "\n",
    "Now, let's discuss when to use each method and create tokenization algorithms by using Python language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076b21b-9054-4a76-9e0f-4430820f1262",
   "metadata": {},
   "source": [
    "## 1. By using NLTK’s word_tokenize() method\n",
    "\n",
    "We use NLTK library to tokenize string into words and punctuation marks. It identifies punctuation as separate tokens, which is essential when the meaning of the text could change depending on punctuation. It is a sophisticated tokenization approach. This method is particularly suitable when dealing with projects that require detailed text analysis.\n",
    "\n",
    "Use case:\n",
    "\n",
    "a. Advanced natural language processing tasks.\\\n",
    "b. When precise tokenization is called-for.\\\n",
    "c. Processing the punctuation marks in text efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3400a45a-f634-40e0-9a7b-ce8f2d7744d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'and', 'Caroline', 'are', 'my', 'good', 'friends']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenization_algorithm1(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text by using NLTK’s word_tokenize() method.\n",
    "\n",
    "    Args:\n",
    "      text: The input string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "      A list of tokens.\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Usage\n",
    "text = \"John and Caroline are my good friends\"\n",
    "tokens = tokenization_algorithm1(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471ba54-740a-4eb5-b52f-91558aa7ff5c",
   "metadata": {},
   "source": [
    "## 2. By using str.split() method\n",
    "\n",
    "This method tokenizes text in DataFrames using the str.split() method. It is suitable for processing huge text data all at once as it allows to tokenize text in an entire column of a Pandas' DataFrame.\n",
    "\n",
    "Use case:\n",
    "\n",
    "a. Processing large amount of text across entire columns.\\\n",
    "b. Dealing with large datasets in DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62c752e-43bd-4c27-b223-204ab2541565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'and', 'Caroline', 'are', 'my', 'good', 'friends']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def tokenization_algorithm2(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text by using str.split() method.\n",
    "\n",
    "    Args:\n",
    "      text: The input string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "      A list of tokens.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"text\": [\"John and Caroline are my good friends\"]})\n",
    "    df['tokens'] = df['text'].str.split()\n",
    "    return (df['tokens'][0])\n",
    "\n",
    "# Usage\n",
    "text = \"John and Caroline are my good friend\"\n",
    "tokens = tokenization_algorithm2(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84369a5c-76b9-41fb-b148-b9dfa57b0f05",
   "metadata": {},
   "source": [
    "## 3. By using Regex with re.findall() method\n",
    "\n",
    "The re.findall() function in Python allows us to extract tokens based on a specific pattern that we define. We can define patterns by using re module. Here, we have complete control over how the text is tokenized.\n",
    "\n",
    "Use case:\n",
    "\n",
    "a. Extracting patterns like email addresses, hashtags, or other custom tokens.\\\n",
    "b. When complete control over token patterns is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82448b2-c5ab-4767-a0be-26c968a99c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'and', 'Caroline', 'are', 'my', 'good', 'friends']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenization_algorithm3(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text by using Regex with re.findall() method.\n",
    "\n",
    "    Args:\n",
    "      text: The input string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "      A list of tokens.\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "# Usage\n",
    "text = \"John and Caroline are my good friends\"\n",
    "tokens = tokenization_algorithm3(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b47880-2f15-4696-a461-1dbe9e986a52",
   "metadata": {},
   "source": [
    "## 4. By using Gensim’s tokenize() method\n",
    "\n",
    "Genism is a useful library in Python, especially useful for  building word vectors and topic modelling. It utilizes tokenize() function to tokenize text. This method integrates seamlessly into Gensim’s environment, facilitating tokenization process in the context of more complicated text analysis.\n",
    "\n",
    "Use case:\n",
    "\n",
    "a. Unification with Gensim’s other functionalities.\\\n",
    "b. While dealing with topic modeling or text processing with Gensim.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba08205-ccef-4b3b-b490-87d91b5df6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'and', 'Caroline', 'are', 'my', 'good', 'friends']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "def tokenization_algorithm4(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text by using Gensim’s tokenize() method.\n",
    "\n",
    "    Args:\n",
    "      text: The input string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "      A list of tokens.\n",
    "    \"\"\"\n",
    "    return list(tokenize(text))\n",
    "\n",
    "# Usage\n",
    "text = \"John and Caroline are my good friends\"\n",
    "tokens = tokenization_algorithm4(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e01ba-cbfb-4d9f-8bc5-1cf5e537a58f",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "Choosing the appropriate tokenization depends on specific requirements, such as processing large datasets, integrating with advanced text analysis mechanism, or handling punctuation in a sentence. By understanding the strength of each method, we can effectively prepare our text data for further analysis and modeling, ensuring that our workflows are coherent, accurate and efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
